# 2021.8.1
吴恩达机器学习系列课程1-1至4.8节

##  监督学习与无监督学习
1. 监督学习
   - 回归问题(Regression problem)
   - 分类问题(Classification problem)
2. 无监督学习
   - 聚类算法
   - 鸡尾酒会问题

## 线性回归算法(Batch)
1. 假设函数: $h_{\theta}(x)$
2. 代价函数：$J(\theta) = \frac{1}{2m}\sum^n_{i=1}(h_{\theta}(x^{(i)}-y^{(i)}) ^2$
3. 梯度下降：$\theta_j := \theta_j - \alpha\frac{\partial J(\theta_0, \theta_1,\cdots, \theta_n)}{\partial \theta_j}$
<br>其中$\alpha$为学习率，控制更新$\theta_j$的幅度。
4. 特征缩放
5. 正规方程解析解：矩阵方法
<br> 求出最优$\vec{\theta}$: $\vec{\theta} = (X^T X)^{-1}X^T\vec{y}$

# 2021.8.2
安装Octave环境；下载编程作业；吴恩达机器学习系列课程5.1-5.3
## Octave语法学习
使用Octave进行矩阵/向量的生成与运算操作。

# 2021.8.3
吴恩达机器学习系列课程5.4-5.6，编程作业1提交
## Matlab绘图、控制语句、函数学习

# 2021.8.4
吴恩达机器学习系列课程6.1-6.7，随堂测试提交
## Logistic Regression
- 0 < $h_\theta(x) < 1$
- $h_\theta = \frac{1}{1+e^{\theta^Tx}}$
- $P(y = 0| x; \theta) + P(y = 1| x; \theta) = 1$
## 决策边界
## 代价函数
- Training Set： ${(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)})\cdots  (x^{(m)}, y^{(m)})}$
- $h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}}$
- Cost function: $J(\theta) = \frac{1}{m}\sum^m_{i = 1}\frac{1}{2}(h_\theta(x^{(i)} - y^{(i)})^2$,——非凸函数，梯度下降不保证得到全局最优
- 变换为凸函数: 
  <br>$Cost(h_\theta(x), y) = -log(h_\theta(x)), y = 1$
  <br>$Cost(h_\theta(x), y) = -log(1 - h_\theta(x)), y = 0$
- 代价函数优化：$Cost(h_\theta(x), y) = ylog(h_\theta(x)) - (1-y)log(1 - h_\theta(x))$
## 最小化代价函数：梯度下降
Repeat{$\theta_j := \theta_j - \alpha\frac{\partial J(\theta_0, \theta_1,\cdots, \theta_n)}{\partial \theta_j}$}
## 高级优化：提高回归速度
Conjugate descent, BFGS, L-BFGS
<br>优点：无需手动选择学习率，算法自动尝试 $\alpha$并选择
## 多元分类：一对多
将某一样本设置为正类别，其余看做负类别，进行多次二元分类

# 2021.8.5
吴恩达机器学习系列课程7.1-8.4，编程作业2
## 过拟合Overfitting
## 正则化
为参数加上惩罚项，使可能导致过拟合的参数尽量小
## 线性回归的正则化
- 梯度下降
<br> $\theta_j = \theta(1-\alpha\frac{\lambda}{m}) - \frac{\alpha}{m}\sum^m_{i = 1}(h_\theta(x^{(i)} - y^{(i)})x_j^{(i)}$
- 解析方法
## 逻辑回归的正则化

## 神经网络
前向传播

# 2021.8.6
吴恩达机器学习系列课程8.5-9.2
## 神经网络的搭建*XOR OR AND XNOR"
## 多类别分类问题
## 神经网络代价函数
反向传播算法

# 2021.8.7
吴恩达机器学习系列课程9.3-9.8
## 反向传播算法
从最后层的误差进行向前误差的推导，得到误差函数关于前面层参数的偏导，是一种快速的求偏导的方式。相关资料：https://www.bilibili.com/video/BV16x411V7Qg?from=search&seid=5931376314277848555
<br>个人理解反向传播算法的结果体现的是结果对前值参数的敏感性，为参数调整提供数值参考
## 展开参数
矩阵 -> 向量：ThetaVec = [Theta1(:); Theta2(:); Theta3(:)]
<br>向量 -> 矩阵：Theta1 = reshape(ThetaVec(1:110), 10, 11) --取前110个元素组成10×11的矩阵；
## 梯度检验
利用传统求偏导方法，求出$J(\theta)$关于参数的偏导，与反向传播函数得出的结果进行比较，在合理范围验证反向传播函数结果的正确性
## 随机初始化
对于权重值$\theta$进行数值随机初始化，避免计算单一特征
## 建立一个神经网络的一些原则
隐藏层的数量：
- 一般为1层
- 多层之间，单元数相等
- 隐藏单元数原则上越多越好，可以为输入的几倍
## 训练一个神经网络的步骤
1. 随机初始化权重
2. 执行前向传播算法
3. 计算代价函数$J(\theta)$
4. 反向传播函数，计算出偏导项$\frac{\partial{J(\theta)}}{\theta}$
5. 梯度检验
6. 停用梯度检验（减少冗余计算），最小化$J(\theta)$
## 神经网络学习应用：自动驾驶

# 2021.8.8
吴恩达机器学习系列课程10.1-10.7
## 学习算法评价
### 数据集分割
Training set: Test set = 7: 3
<br>定义测试的误差函数：
- $error(h_{\theta}(x), y) = 1$, if $h_{\theta}\geq 0.5, y = 0$ or $h_{\theta}(x) < 0.5, y = 1$
- $error(h_{\theta}(x), y) = 0$, otherwise
### 模型选择问题
 数据集分割：
- Training set: 60%, 确定参数
- Cross validation set(cv): 20%， 检验并选择代价最小的模型
- Test set: 20%， 评估，计算泛化误差
### 正则化与高偏差/高方差
正则化参数$\lambda$:
- $\lambda$较大：对参数惩罚较重，可能导致拟合效果不好
- $\lambda$较小：惩罚较小，易导致过拟合
### 利用学习曲线调整模型
1. 高方差错误：
- 更大的训练集
- 更少的特征
- 减小正则化系数$\lambda$
2. 高偏差错误：
- 更多的特征
- 增加多项式次数
- 增大正则化系数$\lambda$
# 2021.8.9
吴恩达机器学习系列课程10.1-10.5
## 监督学习
## 偏斜类问题算法评估
正负样本的数量接近极端时，产生偏斜类问题
<br>使用查准率(Precision)和召回率(Recall)确定算法的合理性
### 查准率与召回率
- Precision = $\frac{True Positive}{True Positive + False Positive}$, 表明预测的准确率
- Recall = $\frac{True Positive}{True Positive + True Negative}$，表明对当前情况做出正确判断的概率
### 结合查准率与召回率进行评估
F score = $2*\frac{Precision*Recall}{Precision + Recall}$
<br>选择F值更大的算法
# 2021.8.10
吴恩达机器学习系列课程12.1-12.6
## 支持向量机SVM
Support Vector Machine, 从logistic regression推导出支持向量机的的代价函数，在不适用核函数的情况下与logistic regression相似，也成为大间距分离器
- SVM目标优化函数：$min_{\theta}\frac{1}{2}\sum^n_{j = 1}\theta_j^2$
- SVM使用大间隔分离数据，具有鲁棒性
## 核函数 Kernels
学习非线性边界
## SVM的偏差/方差折中
- 不使用正则化：低偏差，高方差
- 使用正则化：高偏差，低方差
## 高斯核函数
- $\sigma$较大：更平滑的Gauss曲线
- $\sigma$较小：更陡峭的Gauss曲线
## SVM的实现和内核/算法选择
# 2021.8.11
吴恩达机器学习系列课程13.1-14.2
## 聚类算法Clustering
K均值算法
## 降维Reduction

# 2021.8.12
吴恩达机器学习系列课程14.2-14.7
## PCA算法（主成分分析算法）
第二个无监督算法
算法步骤：
- 数据预处理：均值标准化。均值化归一
- 计算协方差：$\sigma = \frac{1}{m}\sum_{i = 1}^n(x^{(i)})(x^{(i)})^T$
- 计算特征向量：[U,S,V] = svd(Sigma)
## PCA算法中的均值选择
原则：99%的方差被保留
## 原始数据重构
## PCA应用
- 压缩
  - 节省存储空间
  - 加速算法
- 数据可视化

# 2021.8.13
吴恩达机器学习系列课程15.1-15.8
## 异常检测
### 应用
- 异常用户检测
- 大规模生产检测
- 数据中心及其监测
## 高斯分布（正态分布）
X ~ N($\mu, \sigma^2$)
## 异常检测算法
1. 选择可能反映异常的特征量
2. 计算$\mu, \sigma$
3. 得出概率函数P(x)
## 异常检测算法评估
异常检测数据及较倾斜，使用Precision和Recall以及F值来评估算法
## 多元高斯分布
利用协方差矩阵

# 2021.8.14
吴恩达机器学习系列课程16.1-16.6
## 基于内容的推荐算法
在已知内容类型的基础上进行推荐算法
## 协同过滤
基于用户偏好进行类型评分
## 协同过滤算法
结合用户偏好和内容评价同时进行综合最小化
## 协同过滤算法的向量化实现
低秩矩阵分解
## 均值归一化
求平均值，对未被用户评价的内容进行评价，使用平均值

# 2021.8.16
吴恩达机器学习系列课程17.1-19.1
## 随机梯度下降
将样本随机排序，对随机参数进行梯度下降迭代
## Mini-batch gradient descent
将样本分成小规模的样本集，应用合理向量化的方法在小样本集的基础上对参数进行迭代
## 算法检测
利用绘出图像的可视化方法
## 在线学习
不囿于固定的数据集，收集实时的线上数据进行迭代
## Map Reduce 
合理划分算法功能进行多机器并行计算
## OCR
OCR流水线：文本检测->文字切割->特征分类
## 滑动窗口分类器
利用固定比例的窗口进行滑动，并对内容进行标记
## 人工数据合成
## 上线分析
分析流水线中每一步骤进行优化后带来的提升，来确定下一步工作的重点
